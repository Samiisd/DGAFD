{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of learn_tf_text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlnDWObw_jGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tldextract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-XQPR-kq_CFs",
        "colab": {}
      },
      "source": [
        "!git clone https://samiisd:SAMIissaadi94600+@github.com/Samiisd/DGAFD.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNS6ImcZyJxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random as rd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from itertools import chain\n",
        "\n",
        "import sys\n",
        "sys.path.append('./DGAFD')\n",
        "from dga_classifier import data\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.metrics.pairwise import euclidean_distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27qfkkt5yJxs",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Model parameters\n",
        "#@markdown Select the most appropriate parameters.\n",
        "\n",
        "batch_size = 1000 #@param {type: \"slider\", min: 32, max: 1000}\n",
        "steps_per_epoch = 100 #@param {type: \"slider\", min: 1, max: 1000}\n",
        "nb_epochs = 10 #@param {type: \"slider\", min: 1, max: 300}\n",
        "\n",
        "output_dim = 64 #@param {type: \"slider\", min: 16, max: 512}\n",
        "\n",
        "nb_cluster_representant = 3 #@param {type: \"slider\", min: 1, max: 12}\n",
        "\n",
        "#@markdown ---"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeDk5EnKyJxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, data, model, data_initializer, batch_size): # data_initializer, \n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.model = model\n",
        "        self.data = data\n",
        "        self.X = data_initializer(data) # None\n",
        "        self.nb_anchors = len(data)\n",
        "        self.anchors = [None] * self.nb_anchors\n",
        "        self.anchors_exclude = [None] * self.nb_anchors\n",
        "        self.km = KMeans(n_clusters=nb_cluster_representant, n_jobs=-1)\n",
        "        \n",
        "        #self.update()\n",
        "        self.update_anchors()\n",
        "        \n",
        "        for i in range(self.nb_anchors):\n",
        "            self.anchors_exclude[i] = list(chain(range(i),\n",
        "                                                 range(i+1, self.nb_anchors)))\n",
        "    \n",
        "    \n",
        "    def update(self):\n",
        "        self.update_data()\n",
        "        self.update_anchors()\n",
        "        \n",
        "        \n",
        "    def update_data(self):\n",
        "        self.X = self.model(self.data)\n",
        "        \n",
        "        \n",
        "    def update_anchors(self):\n",
        "        for i in range(len(self.anchors)):\n",
        "            self.anchors[i] = pairwise_distances_argmin_min(\n",
        "                self.km.fit(self.X[i]).cluster_centers_, self.X[i])[0]\n",
        "    \n",
        "    def get_anchor(self, i):\n",
        "        return self.anchors[i]\n",
        "        \n",
        "        \n",
        "    def generate_data(self):\n",
        "        # [x-, x, x+], [1, 0]\n",
        "        data = [[], [], []]\n",
        "        classes = np.random.randint(0, len(self.anchors), size=self.batch_size)\n",
        "        for C in classes:\n",
        "            neg_class = rd.choice(self.anchors_exclude[C])\n",
        "            \n",
        "            i_anchor = self.anchors[C][rd.randint(0, nb_cluster_representant-1)]\n",
        "            i_anchor_neg = self.anchors[neg_class][rd.randint(0, nb_cluster_representant-1)]\n",
        "            \n",
        "            data[0].append(self.data[neg_class][i_anchor_neg])\n",
        "            data[1].append(self.data[C][i_anchor])\n",
        "            data[2].append(self.data[C][rd.randint(0, len(self.data[C])-1)])\n",
        "            \n",
        "        data = [np.array(x) for x in data]\n",
        "        \n",
        "        return data, np.array([[1, 0]] * self.batch_size)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.generate_data()\n",
        "\n",
        "    def __len__(self):\n",
        "        return steps_per_epoch\n",
        "    \n",
        "    #def on_epoch_end(self):\n",
        "        #self.update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU1P64Qbb069",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataTranslator():\n",
        "    def __init__(self, labels, domains):\n",
        "        self.chars_map = {x:idx+1 for idx, x in enumerate(set('abcdefghijklmnopqrstuvwxyz0123456789.-'))}\n",
        "        self.chars_map_rev = {idx+1:x for idx, x in enumerate(set('abcdefghijklmnopqrstuvwxyz0123456789.-'))}\n",
        "        self.labels_map = {x:idx for idx, x in enumerate(set(labels))}\n",
        "        self.labels_map_rev = {idx:x for idx, x in enumerate(set(labels))}\n",
        "        \n",
        "        # Numbers the labels\n",
        "        self.labels_num = [self.labels_map[x] for x in labels]\n",
        "        \n",
        "        # Convert domain names to number of sequences\n",
        "        # (+ pad at 64 because this is the max len of a domain names)\n",
        "        self.domains_seq = [[self.chars_map[y] for y in x] for x in domains]\n",
        "        self.domains_seq = DataTranslator.__pad_seq(self.domains_seq)\n",
        "        \n",
        "        self.domains = domains\n",
        "        self.labels = labels\n",
        "\n",
        "    def __pad_seq(seq):\n",
        "        return tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=64)\n",
        "        \n",
        "    def nb_labels(self):\n",
        "        return len(self.labels_map)\n",
        "    \n",
        "    def make_data(self):\n",
        "        # Create the dataset used for the data generation\n",
        "        X = [[] for i in range(self.nb_labels())]\n",
        "        for i in range(len(self.labels_num)):\n",
        "            X[self.labels_num[i]].append(self.domains_seq[i])\n",
        "        return X\n",
        "            \n",
        "    def get_label_name(self, idx):\n",
        "        return self.labels_map_rev.get(idx)\n",
        "    \n",
        "    def get_label_index(self, label_name):\n",
        "        return self.labels_map.get(label_name, -1)\n",
        "    \n",
        "    def domain_to_vec(self, domain_name):\n",
        "        if len(domain_name) > 64:\n",
        "            raise ValueError(\"domain name should contains less than 64 chars\")\n",
        "        translation = None\n",
        "        try:\n",
        "            translation = [self.chars_map[c] for c in domain_name]\n",
        "        except NameError:\n",
        "            raise ValueError(\"given domain name contains unauthorized chars\")\n",
        "\n",
        "        return DataTranslator.__pad_seq([translation])[0]\n",
        "    \n",
        "    def vec_to_domain(self, vec):\n",
        "        str = ''\n",
        "        for c in vec:\n",
        "            str += self.chars_map_rev.get(c, '')\n",
        "        return str\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhy04h2EyJx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels, domains = zip(*data.get_data())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmeSLVrDasix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translator = DataTranslator(labels, domains)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOYjd2sunhgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = translator.make_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOz-tM6yyJyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lossless_triplet_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Implementation of the triplet loss function\n",
        "    \n",
        "    Arguments:\n",
        "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
        "    y_pred -- python list containing three objects:\n",
        "            anchor -- the encodings for the anchor data\n",
        "            positive -- the encodings for the positive data (similar to anchor)\n",
        "            negative -- the encodings for the negative data (different from anchor)\n",
        "    N  --  The number of dimension \n",
        "    beta -- The scaling factor, N is recommended\n",
        "    epsilon -- The Epsilon value to prevent ln(0)\n",
        "    \n",
        "    \n",
        "    Returns:\n",
        "    loss -- real number, value of the loss\n",
        "    \"\"\"\n",
        "    N = output_dim\n",
        "    beta = N\n",
        "    epsilon=1e-8\n",
        "    \n",
        "    negative = tf.convert_to_tensor(y_pred[:,0:N])\n",
        "    anchor = tf.convert_to_tensor(y_pred[:,N:N*2]) \n",
        "    positive = tf.convert_to_tensor(y_pred[:,N*2:N*3])\n",
        "    \n",
        "    # distance between the anchor and the positive\n",
        "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),1)\n",
        "    # distance between the anchor and the negative\n",
        "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),1)\n",
        "    \n",
        "    #Non Linear Values  \n",
        "    \n",
        "    # -ln(-x/N+1)\n",
        "    pos_dist = -tf.log(-tf.divide((pos_dist),beta)+1+epsilon)\n",
        "    neg_dist = -tf.log(-tf.divide((N-neg_dist),beta)+1+epsilon)\n",
        "    \n",
        "    # compute loss\n",
        "    loss = neg_dist + pos_dist\n",
        "    \n",
        "    return loss\n",
        "\n",
        "def build_model(input_dim, embedding_voc_len, alpha=0.25):\n",
        "     # Setting the model input\n",
        "    input_neg = tf.keras.Input(shape=(input_dim,), name='negative') # Input from a different class than the Anchor\n",
        "    input_anc = tf.keras.Input(shape=(input_dim,), name='anchor')   # Input on which comparaison should be done\n",
        "    input_pos = tf.keras.Input(shape=(input_dim,), name='positive') # Input of the same class than the Anchor\n",
        "\n",
        "     # Creation of the Encoder\n",
        "    encoder = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(embedding_voc_len, 64, input_length=input_dim, mask_zero=True),\n",
        "        tf.keras.layers.LSTM(units=64),\n",
        "        tf.keras.layers.Dense(output_dim, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    # Anchor the input with the encoder\n",
        "    encoded_neg = encoder(input_neg)\n",
        "    encoded_anc = encoder(input_anc)\n",
        "    encoded_pos = encoder(input_pos)\n",
        "    \n",
        "    merged = tf.keras.layers.concatenate([encoded_neg, encoded_anc, encoded_pos], axis=-1)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=[input_neg, input_anc, input_pos], outputs=merged)\n",
        "\n",
        "    model.compile(optimizer='adam', loss=lossless_triplet_loss)\n",
        "\n",
        "    return encoder, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2THk-GHyJyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder, model = build_model(64, len(translator.chars_map) + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbl6CO5myJyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X):\n",
        "    pred = [None] * len(X)\n",
        "    for i in range(len(X)):\n",
        "        pred[i] = encoder.predict(np.array(X[i]))\n",
        "    return pred\n",
        "\n",
        "def data_initializer(X):\n",
        "    rnd_init = [None] * len(X)\n",
        "    for i in range(len(X)):\n",
        "        rnd_init[i] = np.random.randn(len(X[i]), output_dim)\n",
        "    return rnd_init\n",
        "\n",
        "dgen = DataGenerator(X, predict, data_initializer, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udhno4BslUO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = dgen.km.predict(dgen.X[11])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tmTljiMlgiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.unique(A, return_counts=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExXc4NRVyJyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "update_anchors_cb = tf.keras.callbacks.LambdaCallback(\n",
        "    on_epoch_end=lambda epoch, logs: dgen.update())\n",
        "\n",
        "learn_history = model.fit_generator(dgen, epochs=nb_epochs, verbose=1, callbacks=[update_anchors_cb])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh0OSAPGyJyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(dgen, epochs=300, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlW9Pa6WyJy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = []\n",
        "nb_pred = 100\n",
        "for data in dgen.data:\n",
        "    pred.append(encoder.predict([data[:nb_pred]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx_qLfutyJzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the weights\n",
        "encoder.save_weights('model_weights.h5')\n",
        "\n",
        "# Save the model architecture\n",
        "with open('model_architecture.json', 'w') as f:\n",
        "    f.write(encoder.to_json())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SWDrxZkyJzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    with tf.device('/cpu:0'):\n",
        "        l = []\n",
        "        u,v=5,2\n",
        "\n",
        "        for i in range(nb_pred):\n",
        "            for j in range(nb_pred):\n",
        "                l.append(pairwise_distances([pred[u][i]], [pred[v][j]])[0][0])\n",
        "        l = np.array(l)\n",
        "        print(\"mean  :\", np.mean(l))\n",
        "        print(\"median:\", np.median(l))\n",
        "        print(\"max   :\", np.max(l))\n",
        "        print(\"min   :\", np.min(l))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0lSg3E7175J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}