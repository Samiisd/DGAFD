{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from dga_classifier import data\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper paramaters\n",
    "batch_size = 500\n",
    "steps_per_epoch = 100\n",
    "nb_epochs = 10\n",
    "\n",
    "output_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, model, data_initializer, batch_size): # data_initializer, \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.X = data_initializer(data) # None\n",
    "        self.nb_anchors = len(data)\n",
    "        self.anchors = [None] * self.nb_anchors\n",
    "        self.anchors_exclude = [None] * self.nb_anchors\n",
    "        self.km = KMeans(n_clusters=1)\n",
    "        \n",
    "        #self.update()\n",
    "        self.update_anchors()\n",
    "        \n",
    "        for i in range(self.nb_anchors):\n",
    "            self.anchors_exclude[i] = list(chain(range(i),\n",
    "                                                 range(i+1, self.nb_anchors)))\n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        self.update_data()\n",
    "        self.update_anchors()\n",
    "        \n",
    "        \n",
    "    def update_data(self):\n",
    "        self.X = self.model(self.data)\n",
    "        \n",
    "        \n",
    "    def update_anchors(self):\n",
    "        for i in range(len(self.anchors)):\n",
    "            self.anchors[i] = pairwise_distances_argmin_min(self.km.fit(self.X[i]).cluster_centers_,\n",
    "                                                            self.X[i])[0][0]\n",
    "    \n",
    "    def get_anchor(self, i):\n",
    "        return self.anchors[i]\n",
    "        \n",
    "        \n",
    "    def generate_data(self):\n",
    "        # [x-, x, x+], [1, 0]\n",
    "        data = [[], [], []]\n",
    "        anchors = np.random.randint(0, len(self.anchors), size=self.batch_size)\n",
    "        for i_anchor in anchors:\n",
    "            i_anchor_neg = rd.choice(self.anchors_exclude[i_anchor])\n",
    "            \n",
    "            data[0].append(self.data[i_anchor_neg][self.anchors[i_anchor_neg]])\n",
    "            data[1].append(self.data[i_anchor][self.anchors[i_anchor]])\n",
    "            data[2].append(self.data[i_anchor][rd.randint(0, len(self.data[i_anchor])-1)])\n",
    "            \n",
    "        data = [np.array(x) for x in data]\n",
    "        \n",
    "        return data, np.array([[1, 0]] * self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.generate_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return steps_per_epoch\n",
    "    \n",
    "    #def on_epoch_end(self):\n",
    "        #self.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, domains = zip(*data.get_data())\n",
    "\n",
    "# Numbers the labels\n",
    "labels_set = set(labels)\n",
    "labels_map = {x:idx for idx, x in enumerate(labels_set)}\n",
    "labels = [labels_map[x] for x in labels]\n",
    "\n",
    "# Convert domain names to number sequences (+ pad at 64 because this is the max len of a domain names)\n",
    "chars_map = {x:idx+1 for idx, x in enumerate(set('abcdefghijklmnopqrstuvwxyz0123456789.-'))}\n",
    "domains = [[chars_map[y] for y in x] for x in domains]\n",
    "domains = tf.keras.preprocessing.sequence.pad_sequences(domains, maxlen=64)\n",
    "\n",
    "assert len(labels) == len(domains)\n",
    "\n",
    "# Create the dataset used for the data generation\n",
    "X = [[] for i in range(len(labels_set))]\n",
    "for i in range(len(labels)):\n",
    "    X[labels[i]].append(domains[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossless_triplet_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss function\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor data\n",
    "            positive -- the encodings for the positive data (similar to anchor)\n",
    "            negative -- the encodings for the negative data (different from anchor)\n",
    "    N  --  The number of dimension \n",
    "    beta -- The scaling factor, N is recommended\n",
    "    epsilon -- The Epsilon value to prevent ln(0)\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    N = output_dim\n",
    "    beta = N\n",
    "    epsilon=1e-8\n",
    "    \n",
    "    negative = tf.convert_to_tensor(y_pred[:,0:N])\n",
    "    anchor = tf.convert_to_tensor(y_pred[:,N:N*2]) \n",
    "    positive = tf.convert_to_tensor(y_pred[:,N*2:N*3])\n",
    "    \n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),1)\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),1)\n",
    "    \n",
    "    #Non Linear Values  \n",
    "    \n",
    "    # -ln(-x/N+1)\n",
    "    pos_dist = -tf.log(-tf.divide((pos_dist),beta)+1+epsilon)\n",
    "    neg_dist = -tf.log(-tf.divide((N-neg_dist),beta)+1+epsilon)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = neg_dist + pos_dist\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def build_model(input_dim, embedding_voc_len, alpha=0.25):\n",
    "     # Setting the model input\n",
    "    input_neg = tf.keras.Input(shape=(input_dim,), name='negative') # Input from a different class than the Anchor\n",
    "    input_anc = tf.keras.Input(shape=(input_dim,), name='anchor')   # Input on which comparaison should be done\n",
    "    input_pos = tf.keras.Input(shape=(input_dim,), name='positive') # Input of the same class than the Anchor\n",
    "\n",
    "     # Creation of the Encoder\n",
    "    encoder = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(embedding_voc_len, 64, input_length=input_dim, mask_zero=True),\n",
    "        tf.keras.layers.LSTM(units=256),\n",
    "        tf.keras.layers.Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Anchor the input with the encoder\n",
    "    encoded_neg = encoder(input_neg)\n",
    "    encoded_anc = encoder(input_anc)\n",
    "    encoded_pos = encoder(input_pos)\n",
    "    \n",
    "    merged = tf.keras.layers.concatenate([encoded_neg, encoded_anc, encoded_pos], axis=-1)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_neg, input_anc, input_pos], outputs=merged)\n",
    "\n",
    "    model.compile(optimizer='adam', loss=lossless_triplet_loss)\n",
    "\n",
    "    return encoder, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/white/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "encoder, model = build_model(64, len(chars_map) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    pred = [None] * len(X)\n",
    "    for i in range(len(X)):\n",
    "        pred[i] = encoder.predict(np.array(X[i]))\n",
    "    return pred\n",
    "\n",
    "def data_initializer(X):\n",
    "    rnd_init = [None] * len(X)\n",
    "    for i in range(len(X)):\n",
    "        rnd_init[i] = np.random.randn(len(X[i]), output_dim)\n",
    "    return rnd_init\n",
    "\n",
    "dgen = DataGenerator(X, predict, data_initializer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/white/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "  1/100 [..............................] - ETA: 8:30 - loss: 13.1493"
     ]
    }
   ],
   "source": [
    "update_anchors_cb = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: dgen.update())\n",
    "\n",
    "learn_history = model.fit_generator(dgen, epochs=nb_epochs, verbose=1, callbacks=[update_anchors_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(dgen, epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "nb_pred = 100\n",
    "for data in dgen.data:\n",
    "    pred.append(encoder.predict([data[:nb_pred]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "encoder.save_weights('model_weights.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(encoder.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.device('/cpu:0'):\n",
    "        l = []\n",
    "        u,v=2,2\n",
    "        for i in range(nb_pred):\n",
    "            for j in range(nb_pred):\n",
    "                l.append(pairwise_distances([pred[u][i]], [pred[v][j]])[0][0])\n",
    "        l = np.array(l)\n",
    "        print(\"mean  :\", np.mean(l))\n",
    "        print(\"median:\", np.median(l))\n",
    "        print(\"max   :\", np.max(l))\n",
    "        print(\"min   :\", np.min(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
