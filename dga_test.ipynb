{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNS6ImcZyJxd"
   },
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "\n",
    "from keras.engine.training_utils import standardize_input_data\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from dga_classifier import data\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "27qfkkt5yJxs"
   },
   "outputs": [],
   "source": [
    "#@title Model parameters\n",
    "#@markdown Select the most appropriate parameters.\n",
    "\n",
    "nb_data_to_generate = 10000 #@param {type: \"slider\", min: 100, max:20000}\n",
    "\n",
    "batch_size = 500 #@param {type: \"slider\", min: 32, max: 1000}\n",
    "steps_per_epoch = 256 #@param {type: \"slider\", min: 1, max: 1000}\n",
    "nb_epochs = 20 #@param {type: \"slider\", min: 1, max: 300}\n",
    "\n",
    "output_dim = 128 #@param {type: \"slider\", min: 16, max: 512}\n",
    "\n",
    "nb_cluster_representant = 1 #@param {type: \"slider\", min: 1, max: 12}\n",
    "\n",
    "nb_embedding_data = 2000 #@param {type: \"slider\", min: 64, max: 8192}\n",
    "batch_size_emb = 100 #@param {type: \"slider\", min: 32, max: 1000}\n",
    "\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MeDk5EnKyJxz"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, model, data_initializer, batch_size): # data_initializer, \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.X = data_initializer(data) # None\n",
    "        self.nb_anchors = len(data)\n",
    "        self.anchors = [None] * self.nb_anchors\n",
    "        self.anchors_exclude = [None] * self.nb_anchors\n",
    "        self.km = KMeans(n_clusters=nb_cluster_representant, n_jobs=-1)\n",
    "        \n",
    "        #self.update()\n",
    "        self.update_anchors()\n",
    "        \n",
    "        for i in range(self.nb_anchors):\n",
    "            self.anchors_exclude[i] = list(chain(range(i),\n",
    "                                                 range(i+1, self.nb_anchors)))\n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        self.update_data()\n",
    "        self.update_anchors()\n",
    "        \n",
    "        \n",
    "    def update_data(self):\n",
    "        self.X = self.model(self.data)\n",
    "        \n",
    "        \n",
    "    def update_anchors(self):\n",
    "        for i in range(len(self.anchors)):\n",
    "            self.anchors[i] = pairwise_distances_argmin_min(\n",
    "                self.km.fit(self.X[i]).cluster_centers_, self.X[i])[0]\n",
    "    \n",
    "    def get_anchor(self, i):\n",
    "        return self.anchors[i]\n",
    "        \n",
    "        \n",
    "    def generate_data(self):\n",
    "        # [x-, x, x+], [1, 0]\n",
    "        data = [[], [], []]\n",
    "        classes = np.random.randint(0, len(self.anchors), size=self.batch_size)\n",
    "        for C in classes:\n",
    "            neg_class = rd.choice(self.anchors_exclude[C])\n",
    "            \n",
    "            i_anchor = self.anchors[C][rd.randint(0, nb_cluster_representant-1)]\n",
    "            i_anchor_neg = self.anchors[neg_class][rd.randint(0, nb_cluster_representant-1)]\n",
    "            \n",
    "            data[0].append(self.data[neg_class][i_anchor_neg])\n",
    "            data[1].append(self.data[C][i_anchor])\n",
    "            data[2].append(self.data[C][rd.randint(0, len(self.data[C])-1)])\n",
    "            \n",
    "        data = [np.array(x) for x in data]\n",
    "        \n",
    "        return data, np.array([[1, 0]] * self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.generate_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return steps_per_epoch\n",
    "    \n",
    "    #def on_epoch_end(self):\n",
    "        #self.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZU1P64Qbb069"
   },
   "outputs": [],
   "source": [
    "class DataTranslator():\n",
    "    def __init__(self, labels, domains):\n",
    "        self.chars_map = {x:idx+1 for idx, x in enumerate(set('abcdefghijklmnopqrstuvwxyz0123456789.-'))}\n",
    "        self.chars_map_rev = {idx+1:x for idx, x in enumerate(set('abcdefghijklmnopqrstuvwxyz0123456789.-'))}\n",
    "        self.labels_map = {x:idx for idx, x in enumerate(set(labels))}\n",
    "        self.labels_map_rev = {idx:x for idx, x in enumerate(set(labels))}\n",
    "        \n",
    "        # Numbers the labels\n",
    "        self.labels_num = [self.labels_map[x] for x in labels]\n",
    "        \n",
    "        # Convert domain names to number of sequences\n",
    "        # (+ pad at 64 because this is the max len of a domain names)\n",
    "        self.domains_seq = [[self.chars_map[y] for y in x] for x in domains]\n",
    "        self.domains_seq = DataTranslator.__pad_seq(self.domains_seq)\n",
    "        \n",
    "        self.domains = domains\n",
    "        self.labels = labels\n",
    "\n",
    "    def __pad_seq(seq):\n",
    "        return tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=64)\n",
    "        \n",
    "    def nb_labels(self):\n",
    "        return len(self.labels_map)\n",
    "    \n",
    "    def make_data(self):\n",
    "        # Create the dataset used for the data generation\n",
    "        X = [[] for i in range(self.nb_labels())]\n",
    "        for i in range(len(self.labels_num)):\n",
    "            X[self.labels_num[i]].append(self.domains_seq[i])\n",
    "        return X\n",
    "            \n",
    "    def get_label_name(self, idx):\n",
    "        return self.labels_map_rev.get(idx)\n",
    "    \n",
    "    def get_label_index(self, label_name):\n",
    "        return self.labels_map.get(label_name, -1)\n",
    "    \n",
    "    def domain_to_vec(self, domain_name):\n",
    "        if len(domain_name) > 64:\n",
    "            raise ValueError(\"domain name should contains less than 64 chars\")\n",
    "        translation = None\n",
    "        try:\n",
    "            translation = [self.chars_map[c] for c in domain_name]\n",
    "        except NameError:\n",
    "            raise ValueError(\"given domain name contains unauthorized chars\")\n",
    "\n",
    "        return DataTranslator.__pad_seq([translation])[0]\n",
    "    \n",
    "    def vec_to_domain(self, vec):\n",
    "        str = ''\n",
    "        for c in vec:\n",
    "            str += self.chars_map_rev.get(c, '')\n",
    "        return str\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vhy04h2EyJx8"
   },
   "outputs": [],
   "source": [
    "labels, domains = zip(*data.get_data(nb_data_to_generate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KmeSLVrDasix"
   },
   "outputs": [],
   "source": [
    "translator = DataTranslator(labels, domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOYjd2sunhgM"
   },
   "outputs": [],
   "source": [
    "X = translator.make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOz-tM6yyJyD"
   },
   "outputs": [],
   "source": [
    "def lossless_triplet_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss function\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor data\n",
    "            positive -- the encodings for the positive data (similar to anchor)\n",
    "            negative -- the encodings for the negative data (different from anchor)\n",
    "    N  --  The number of dimension \n",
    "    beta -- The scaling factor, N is recommended\n",
    "    epsilon -- The Epsilon value to prevent ln(0)\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    N = output_dim\n",
    "    beta = N\n",
    "    epsilon=1e-8\n",
    "    \n",
    "    negative = tf.convert_to_tensor(y_pred[:,0:N])\n",
    "    anchor = tf.convert_to_tensor(y_pred[:,N:N*2]) \n",
    "    positive = tf.convert_to_tensor(y_pred[:,N*2:N*3])\n",
    "    \n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),1)\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),1)\n",
    "    \n",
    "    #Non Linear Values  \n",
    "    \n",
    "    # -ln(-x/N+1)\n",
    "    pos_dist = -tf.log(-tf.divide((pos_dist),beta)+1+epsilon)\n",
    "    neg_dist = -tf.log(-tf.divide((N-neg_dist),beta)+1+epsilon)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = neg_dist + pos_dist\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def build_model(input_dim, embedding_voc_len, alpha=0.25):\n",
    "     # Setting the model input\n",
    "    input_neg = tf.keras.Input(shape=(input_dim,), name='negative') # Input from a different class than the Anchor\n",
    "    input_anc = tf.keras.Input(shape=(input_dim,), name='anchor')   # Input on which comparaison should be done\n",
    "    input_pos = tf.keras.Input(shape=(input_dim,), name='positive') # Input of the same class than the Anchor\n",
    "\n",
    "     # Creation of the Encoder\n",
    "    encoder = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(embedding_voc_len, 64, input_length=input_dim, mask_zero=True),\n",
    "        tf.keras.layers.LSTM(units=64),\n",
    "        tf.keras.layers.Dense(output_dim, activation='sigmoid', name='custom_embedding')\n",
    "    ])\n",
    "    \n",
    "    # Anchor the input with the encoder\n",
    "    encoded_neg = encoder(input_neg)\n",
    "    encoded_anc = encoder(input_anc)\n",
    "    encoded_pos = encoder(input_pos)\n",
    "    \n",
    "    merged = tf.keras.layers.concatenate([encoded_neg, encoded_anc, encoded_pos], axis=-1)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_neg, input_anc, input_pos], outputs=merged)\n",
    "\n",
    "    model.compile(optimizer='adam', loss=lossless_triplet_loss)\n",
    "\n",
    "    return encoder, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r2THk-GHyJyK"
   },
   "outputs": [],
   "source": [
    "encoder, model = build_model(64, len(translator.chars_map) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xbl6CO5myJyZ"
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    pred = [None] * len(X)\n",
    "    for i in range(len(X)):\n",
    "        pred[i] = encoder.predict(np.array(X[i]))\n",
    "    return pred\n",
    "\n",
    "def data_initializer(X):\n",
    "    rnd_init = [None] * len(X)\n",
    "    for i in range(len(X)):\n",
    "        rnd_init[i] = np.random.randn(len(X[i]), output_dim)\n",
    "    return rnd_init\n",
    "\n",
    "dgen = DataGenerator(X, predict, data_initializer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_indices = np.random.randint(0, len(translator.labels), nb_embedding_data)\n",
    "x_test_emb = [[translator.domains_seq[i] for i in rnd_indices]]\n",
    "y_test_emb = [translator.labels[i] for i in rnd_indices] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = \"./logs\"\n",
    "embedding_metadata = \"emb_metadata.tsv\"\n",
    "with open(os.path.join(logs_dir, embedding_metadata), \"wb\") as f:\n",
    "    np.savetxt(f, y_test_emb, \"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emb_projector(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, model, X, metadata_path, batch_size=32, freq=1, logs_dir='./logs'):\n",
    "        self.sess = K.get_session()\n",
    "        \n",
    "        self.freq = freq\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.x_std = standardize_input_data(X, model.input_names)\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter(logs_dir)\n",
    "        self.logs_dir = logs_dir\n",
    "        \n",
    "        self.metadata_path = metadata_path\n",
    "        \n",
    "    def set_model(self, model, layer):\n",
    "        self.model = model\n",
    "        \n",
    "        self.batch_id = batch_id = tf.placeholder(tf.int32)\n",
    "        self.step = step = tf.placeholder(tf.int32)\n",
    "        \n",
    "        # --- Creation of the embedding layer ---\n",
    "        emb_input = layer.output\n",
    "        emb_size = np.prod(emb_input.shape[1:])\n",
    "        emb_input = tf.reshape(emb_input, (step, int(emb_size)))\n",
    "\n",
    "        shape = (self.x_std[0].shape[0], int(emb_size)) \n",
    "        self.emb_var = tf.Variable(tf.zeros(shape), name=layer.name + \"_embedding\")\n",
    "        self.assign_emb = tf.assign(self.emb_var[batch_id:batch_id+step], emb_input)\n",
    "        \n",
    "        self.saver = tf.train.Saver([self.emb_var])\n",
    "        \n",
    "        config = projector.ProjectorConfig()\n",
    "        \n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = self.emb_var.name\n",
    "        embedding.metadata_path = self.metadata_path\n",
    "        \n",
    "        projector.visualize_embeddings(self.writer, config)\n",
    "        \n",
    "        self.sess.run(self.emb_var.initializer)\n",
    "\n",
    "        \n",
    "    def log_emb(self, checkpoint_name=''):\n",
    "        n_samples = self.x_std[0].shape[0]\n",
    "        \n",
    "        i = 0\n",
    "        while i < n_samples:\n",
    "            step = min(self.batch_size, n_samples - i)\n",
    "            batch = slice(i, i+step)\n",
    "            \n",
    "            feed_dict = {self.model.input: self.x_std[0][batch]}\n",
    "            \n",
    "            feed_dict.update({self.batch_id: i, self.step: step})\n",
    "            \n",
    "            self.sess.run(self.assign_emb, feed_dict=feed_dict)\n",
    "            self.saver.save(self.sess,\n",
    "                           os.path.join(self.logs_dir, 'emb_checkpoint_' + checkpoint_name + '.ckpt'))\n",
    "            \n",
    "            i += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_proj = emb_projector(encoder, x_test_emb, embedding_metadata, batch_size_emb, 5, logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExXc4NRVyJyg"
   },
   "outputs": [],
   "source": [
    "def proj_update(epoch, logs):\n",
    "    emb_proj.set_model(encoder,  encoder.get_layer('custom_embedding'))\n",
    "    emb_proj.log_emb(str(epoch))\n",
    "    \n",
    "update_anchors_cb = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: dgen.update())\n",
    "\n",
    "projector_cb = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: proj_update(epoch, logs))\n",
    "\n",
    "\n",
    "learn_history = model.fit_generator(dgen, epochs=nb_epochs, verbose=1, callbacks=[update_anchors_cb, projector_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oh0OSAPGyJyr"
   },
   "outputs": [],
   "source": [
    "model.fit_generator(dgen, epochs=100, verbose=1, callbacks=[projector_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlW9Pa6WyJy5"
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "nb_pred = 100\n",
    "for data in dgen.data:\n",
    "    pred.append(encoder.predict([data[:nb_pred]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bx_qLfutyJzB"
   },
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "encoder.save_weights('model_weights.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(encoder.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SWDrxZkyJzI"
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "u,v=5,1\n",
    "\n",
    "for i in range(nb_pred):\n",
    "    for j in range(nb_pred):\n",
    "        l.append(pairwise_distances([pred[u][i]], [pred[v][j]])[0][0])\n",
    "l = np.array(l)\n",
    "print(\"mean  :\", np.mean(l))\n",
    "print(\"median:\", np.median(l))\n",
    "print(\"max   :\", np.max(l))\n",
    "print(\"min   :\", np.min(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0lSg3E7175J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of learn_tf_text.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
