{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNS6ImcZyJxd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/requests-2.21.0-py3.7.egg/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from keras.engine.training_utils import standardize_input_data\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from dga_classifier import data\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "27qfkkt5yJxs"
   },
   "outputs": [],
   "source": [
    "#@title Model parameters\n",
    "#@markdown Select the most appropriate parameters.\n",
    "\n",
    "nb_data_to_generate = 1000 #@param {type: \"slider\", min: 100, max:20000}\n",
    "\n",
    "batch_size = 500 #@param {type: \"slider\", min: 32, max: 1000}\n",
    "steps_per_epoch = 100 #@param {type: \"slider\", min: 1, max: 1000}\n",
    "nb_epochs = 10 #@param {type: \"slider\", min: 1, max: 300}\n",
    "\n",
    "output_dim = 128 #@param {type: \"slider\", min: 16, max: 512}\n",
    "\n",
    "nb_cluster_representant = 1 #@param {type: \"slider\", min: 1, max: 12}\n",
    "\n",
    "nb_embedding_data = 1000 #@param {type: \"slider\", min: 64, max: 8192}\n",
    "batch_size_emb = 100 #@param {type: \"slider\", min: 32, max: 1000}\n",
    "\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MeDk5EnKyJxz"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, model, data_initializer, batch_size): # data_initializer, \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.X = data_initializer(data) # None\n",
    "        self.nb_anchors = len(data)\n",
    "        self.anchors = [None] * self.nb_anchors\n",
    "        self.anchors_exclude = [None] * self.nb_anchors\n",
    "        self.km = KMeans(n_clusters=nb_cluster_representant, n_jobs=-1)\n",
    "        \n",
    "        #self.update()\n",
    "        self.update_anchors()\n",
    "        \n",
    "        for i in range(self.nb_anchors):\n",
    "            self.anchors_exclude[i] = list(chain(range(i),\n",
    "                                                 range(i+1, self.nb_anchors)))\n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        self.update_data()\n",
    "        self.update_anchors()\n",
    "        \n",
    "        \n",
    "    def update_data(self):\n",
    "        self.X = self.model(self.data)\n",
    "        \n",
    "        \n",
    "    def update_anchors(self):\n",
    "        for i in range(len(self.anchors)):\n",
    "            self.anchors[i] = pairwise_distances_argmin_min(\n",
    "                self.km.fit(self.X[i]).cluster_centers_, self.X[i])[0]\n",
    "    \n",
    "    def get_anchor(self, i):\n",
    "        return self.anchors[i]\n",
    "        \n",
    "        \n",
    "    def generate_data(self):\n",
    "        # [x-, x, x+], [1, 0]\n",
    "        data = [[], [], []]\n",
    "        classes = np.random.randint(0, len(self.anchors), size=self.batch_size)\n",
    "        for C in classes:\n",
    "            neg_class = rd.choice(self.anchors_exclude[C])\n",
    "            \n",
    "            i_anchor = self.anchors[C][rd.randint(0, nb_cluster_representant-1)]\n",
    "            i_anchor_neg = self.anchors[neg_class][rd.randint(0, nb_cluster_representant-1)]\n",
    "            \n",
    "            data[0].append(self.data[neg_class][i_anchor_neg])\n",
    "            data[1].append(self.data[C][i_anchor])\n",
    "            data[2].append(self.data[C][rd.randint(0, len(self.data[C])-1)])\n",
    "            \n",
    "        data = [np.array(x) for x in data]\n",
    "        \n",
    "        return data, np.array([[1, 0]] * self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.generate_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return steps_per_epoch\n",
    "    \n",
    "    #def on_epoch_end(self):\n",
    "        #self.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZU1P64Qbb069"
   },
   "outputs": [],
   "source": [
    "class DataTranslator():\n",
    "    def __init__(self, labels, domains):\n",
    "        self.chars_map = {x:idx+1 for idx, x in enumerate(set('abcdefghijklmnopqrstuvwxyz0123456789.-'))}\n",
    "        self.chars_map_rev = {idx+1:x for idx, x in enumerate(set('abcdefghijklmnopqrstuvwxyz0123456789.-'))}\n",
    "        self.labels_map = {x:idx for idx, x in enumerate(set(labels))}\n",
    "        self.labels_map_rev = {idx:x for idx, x in enumerate(set(labels))}\n",
    "        \n",
    "        # Numbers the labels\n",
    "        self.labels_num = [self.labels_map[x] for x in labels]\n",
    "        \n",
    "        # Convert domain names to number of sequences\n",
    "        # (+ pad at 64 because this is the max len of a domain names)\n",
    "        self.domains_seq = [[self.chars_map[y] for y in x] for x in domains]\n",
    "        self.domains_seq = DataTranslator.__pad_seq(self.domains_seq)\n",
    "        \n",
    "        self.domains = domains\n",
    "        self.labels = labels\n",
    "\n",
    "    def __pad_seq(seq):\n",
    "        return tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=64)\n",
    "        \n",
    "    def nb_labels(self):\n",
    "        return len(self.labels_map)\n",
    "    \n",
    "    def make_data(self):\n",
    "        # Create the dataset used for the data generation\n",
    "        X = [[] for i in range(self.nb_labels())]\n",
    "        for i in range(len(self.labels_num)):\n",
    "            X[self.labels_num[i]].append(self.domains_seq[i])\n",
    "        return X\n",
    "            \n",
    "    def get_label_name(self, idx):\n",
    "        return self.labels_map_rev.get(idx)\n",
    "    \n",
    "    def get_label_index(self, label_name):\n",
    "        return self.labels_map.get(label_name, -1)\n",
    "    \n",
    "    def domain_to_vec(self, domain_name):\n",
    "        if len(domain_name) > 64:\n",
    "            raise ValueError(\"domain name should contains less than 64 chars\")\n",
    "        translation = None\n",
    "        try:\n",
    "            translation = [self.chars_map[c] for c in domain_name]\n",
    "        except NameError:\n",
    "            raise ValueError(\"given domain name contains unauthorized chars\")\n",
    "\n",
    "        return DataTranslator.__pad_seq([translation])[0]\n",
    "    \n",
    "    def vec_to_domain(self, vec):\n",
    "        str = ''\n",
    "        for c in vec:\n",
    "            str += self.chars_map_rev.get(c, '')\n",
    "        return str\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vhy04h2EyJx8"
   },
   "outputs": [],
   "source": [
    "labels, domains = zip(*data.get_data(nb_data_to_generate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KmeSLVrDasix"
   },
   "outputs": [],
   "source": [
    "translator = DataTranslator(labels, domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOYjd2sunhgM"
   },
   "outputs": [],
   "source": [
    "X = translator.make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOz-tM6yyJyD"
   },
   "outputs": [],
   "source": [
    "def lossless_triplet_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss function\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor data\n",
    "            positive -- the encodings for the positive data (similar to anchor)\n",
    "            negative -- the encodings for the negative data (different from anchor)\n",
    "    N  --  The number of dimension \n",
    "    beta -- The scaling factor, N is recommended\n",
    "    epsilon -- The Epsilon value to prevent ln(0)\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    N = output_dim\n",
    "    beta = N\n",
    "    epsilon=1e-8\n",
    "    \n",
    "    negative = tf.convert_to_tensor(y_pred[:,0:N])\n",
    "    anchor = tf.convert_to_tensor(y_pred[:,N:N*2]) \n",
    "    positive = tf.convert_to_tensor(y_pred[:,N*2:N*3])\n",
    "    \n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),1)\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),1)\n",
    "    \n",
    "    #Non Linear Values  \n",
    "    \n",
    "    # -ln(-x/N+1)\n",
    "    pos_dist = -tf.log(-tf.divide((pos_dist),beta)+1+epsilon)\n",
    "    neg_dist = -tf.log(-tf.divide((N-neg_dist),beta)+1+epsilon)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = neg_dist + pos_dist\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def build_model(input_dim, embedding_voc_len, alpha=0.25):\n",
    "     # Setting the model input\n",
    "    input_neg = tf.keras.Input(shape=(input_dim,), name='negative') # Input from a different class than the Anchor\n",
    "    input_anc = tf.keras.Input(shape=(input_dim,), name='anchor')   # Input on which comparaison should be done\n",
    "    input_pos = tf.keras.Input(shape=(input_dim,), name='positive') # Input of the same class than the Anchor\n",
    "\n",
    "     # Creation of the Encoder\n",
    "    encoder = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(embedding_voc_len, 64, input_length=input_dim, mask_zero=True),\n",
    "        tf.keras.layers.LSTM(units=64),\n",
    "        tf.keras.layers.Dense(output_dim, activation='sigmoid', name='custom_embedding')\n",
    "    ])\n",
    "    \n",
    "    # Anchor the input with the encoder\n",
    "    encoded_neg = encoder(input_neg)\n",
    "    encoded_anc = encoder(input_anc)\n",
    "    encoded_pos = encoder(input_pos)\n",
    "    \n",
    "    merged = tf.keras.layers.concatenate([encoded_neg, encoded_anc, encoded_pos], axis=-1)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_neg, input_anc, input_pos], outputs=merged)\n",
    "\n",
    "    model.compile(optimizer='adam', loss=lossless_triplet_loss)\n",
    "\n",
    "    return encoder, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r2THk-GHyJyK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/supra/venv_ml/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "encoder, model = build_model(64, len(translator.chars_map) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xbl6CO5myJyZ"
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    pred = [None] * len(X)\n",
    "    for i in range(len(X)):\n",
    "        pred[i] = encoder.predict(np.array(X[i]))\n",
    "    return pred\n",
    "\n",
    "def data_initializer(X):\n",
    "    rnd_init = [None] * len(X)\n",
    "    for i in range(len(X)):\n",
    "        rnd_init[i] = np.random.randn(len(X[i]), output_dim)\n",
    "    return rnd_init\n",
    "\n",
    "dgen = DataGenerator(X, predict, data_initializer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_indices = np.random.randint(0, len(translator.labels), nb_embedding_data)\n",
    "x_test_emb = [[translator.domains_seq[i] for i in rnd_indices]]\n",
    "y_test_emb = [translator.labels[i] for i in rnd_indices] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = \"./logs\"\n",
    "embedding_metadata = \"emb_metadata.tsv\"\n",
    "with open(os.path.join(logs_dir, embedding_metadata), \"wb\") as f:\n",
    "    np.savetxt(f, y_test_emb, \"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emb_projector(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, model, layer, X, metadata_path, batch_size=32, freq=1, logs_dir='./logs'):\n",
    "        self.sess = K.get_session()\n",
    "        \n",
    "        self.freq = freq\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.x_std = standardize_input_data(X, model.input_names)\n",
    "        \n",
    "        self.batch_id = batch_id = tf.placeholder(tf.int32)\n",
    "        self.step = step = tf.placeholder(tf.int32)\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter(logs_dir)\n",
    "        self.logs_dir = logs_dir\n",
    "        \n",
    "\n",
    "        # --- Creation of the embedding layer ---\n",
    "        emb_input = layer.output\n",
    "        emb_size = np.prod(emb_input.shape[1:])\n",
    "        emb_input = tf.reshape(emb_input, (step, int(emb_size)))\n",
    "\n",
    "        shape = (self.x_std[0].shape[0], int(emb_size)) \n",
    "        self.emb_var = tf.Variable(tf.zeros(shape), name=layer.name + \"_embedding\")\n",
    "        self.assign_emb = tf.assign(self.emb_var[batch_id:batch_id+step], emb_input)\n",
    "        \n",
    "        self.saver = tf.train.Saver([self.emb_var])\n",
    "        \n",
    "        config = projector.ProjectorConfig()\n",
    "        \n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = self.emb_var.name\n",
    "        embedding.metadata_path = metadata_path\n",
    "        \n",
    "        projector.visualize_embeddings(self.writer, config)\n",
    "        \n",
    "        # cause errors\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)    \n",
    "        \n",
    "    def log_emb(self, model, checkpoint_name=''):\n",
    "        n_samples = self.x_std[0].shape[0]\n",
    "        \n",
    "        i = 0\n",
    "        while i < n_samples:\n",
    "            step = min(self.batch_size, n_samples - i)\n",
    "            batch = slice(i, i+step)\n",
    "            \n",
    "            feed_dict = {model.input: self.x_std[0][batch]}\n",
    "            \n",
    "            feed_dict.update({self.batch_id: i, self.step: step})\n",
    "            \n",
    "            self.sess.run(self.assign_emb, feed_dict=feed_dict)\n",
    "            self.saver.save(self.sess,\n",
    "                           os.path.join(self.logs_dir, 'emb_checkpoint_' + checkpoint_name + '.ckpt'))\n",
    "            \n",
    "            i += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_proj = emb_projector(encoder, encoder.get_layer('custom_embedding'), x_test_emb, embedding_metadata, batch_size_emb, 5, logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExXc4NRVyJyg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 4.9097\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 2.2425\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 1.9223\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 1.9045\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 1.7740\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 51s 506ms/step - loss: 1.6991\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 1.7919\n",
      "Epoch 8/10\n",
      " 97/100 [============================>.] - ETA: 1s - loss: 1.8012"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-1d51850a8380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlearn_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_anchors_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojector_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv_ml/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/venv_ml/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_ml/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_ml/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/venv_ml/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "update_anchors_cb = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: dgen.update())\n",
    "\n",
    "projector_cb = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: emb_proj.log_emb(encoder, str(epoch)))\n",
    "\n",
    "\n",
    "learn_history = model.fit_generator(dgen, epochs=nb_epochs, verbose=1, callbacks=[update_anchors_cb, projector_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oh0OSAPGyJyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 74/100 [=====================>........] - ETA: 8s - loss: 5.7858"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2dcb9d72130a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprojector_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv_ml/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/venv_ml/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_ml/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_ml/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/venv_ml/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(dgen, epochs=100, verbose=1, callbacks=[projector_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_proj.log_emb('mega')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlW9Pa6WyJy5"
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "nb_pred = 100\n",
    "for data in dgen.data:\n",
    "    pred.append(encoder.predict([data[:nb_pred]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bx_qLfutyJzB"
   },
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "encoder.save_weights('model_weights.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(encoder.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SWDrxZkyJzI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean  : 0.020354623\n",
      "median: 0.020216018\n",
      "max   : 0.036187273\n",
      "min   : 0.007041768\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.device('/cpu:0'):\n",
    "        l = []\n",
    "        u,v=5,9\n",
    "\n",
    "        for i in range(nb_pred):\n",
    "            for j in range(nb_pred):\n",
    "                l.append(pairwise_distances([pred[u][i]], [pred[v][j]])[0][0])\n",
    "        l = np.array(l)\n",
    "        print(\"mean  :\", np.mean(l))\n",
    "        print(\"median:\", np.median(l))\n",
    "        print(\"max   :\", np.max(l))\n",
    "        print(\"min   :\", np.min(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0lSg3E7175J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of learn_tf_text.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
