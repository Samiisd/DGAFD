{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNS6ImcZyJxd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "\n",
    "from keras.engine.training_utils import standardize_input_data\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from dga_classifier import data\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "27qfkkt5yJxs"
   },
   "outputs": [],
   "source": [
    "#@title Model parameters\n",
    "#@markdown Select the most appropriate parameters.\n",
    "\n",
    "nb_data_to_generate = 10000 #@param {type: \"slider\", min: 100, max:20000}\n",
    "\n",
    "batch_size = 500 #@param {type: \"slider\", min: 32, max: 1000}\n",
    "steps_per_epoch = 256 #@param {type: \"slider\", min: 1, max: 1000}\n",
    "nb_epochs = 20 #@param {type: \"slider\", min: 1, max: 300}\n",
    "\n",
    "output_dim = 128 #@param {type: \"slider\", min: 16, max: 512}\n",
    "\n",
    "nb_cluster_representant = 1 #@param {type: \"slider\", min: 1, max: 12}\n",
    "\n",
    "nb_embedding_data = 2000 #@param {type: \"slider\", min: 64, max: 8192}\n",
    "batch_size_emb = 100 #@param {type: \"slider\", min: 32, max: 1000}\n",
    "\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MeDk5EnKyJxz"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, model, data_initializer, batch_size): # data_initializer, \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.X = data_initializer(data) # None\n",
    "        self.nb_anchors = len(data)\n",
    "        self.anchors = [None] * self.nb_anchors\n",
    "        self.anchors_exclude = [None] * self.nb_anchors\n",
    "        self.km = KMeans(n_clusters=nb_cluster_representant, n_jobs=-1)\n",
    "        \n",
    "        #self.update()\n",
    "        self.update_anchors()\n",
    "        \n",
    "        for i in range(self.nb_anchors):\n",
    "            self.anchors_exclude[i] = list(chain(range(i),\n",
    "                                                 range(i+1, self.nb_anchors)))\n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        self.update_data()\n",
    "        self.update_anchors()\n",
    "        \n",
    "        \n",
    "    def update_data(self):\n",
    "        self.X = self.model(self.data)\n",
    "        \n",
    "        \n",
    "    def update_anchors(self):\n",
    "        for i in range(len(self.anchors)):\n",
    "            self.anchors[i] = pairwise_distances_argmin_min(\n",
    "                self.km.fit(self.X[i]).cluster_centers_, self.X[i])[0]\n",
    "    \n",
    "    def get_anchor(self, i):\n",
    "        return self.anchors[i]\n",
    "        \n",
    "        \n",
    "    def generate_data(self):\n",
    "        # [x-, x, x+], [1, 0]\n",
    "        data = [[], [], []]\n",
    "        classes = np.random.randint(0, len(self.anchors), size=self.batch_size)\n",
    "        for C in classes:\n",
    "            neg_class = rd.choice(self.anchors_exclude[C])\n",
    "            \n",
    "            i_anchor = self.anchors[C][rd.randint(0, nb_cluster_representant-1)]\n",
    "            i_anchor_neg = self.anchors[neg_class][rd.randint(0, nb_cluster_representant-1)]\n",
    "            \n",
    "            data[0].append(self.data[neg_class][i_anchor_neg])\n",
    "            data[1].append(self.data[C][i_anchor])\n",
    "            data[2].append(self.data[C][rd.randint(0, len(self.data[C])-1)])\n",
    "            \n",
    "        data = [np.array(x) for x in data]\n",
    "        \n",
    "        return data, np.array([[1, 0]] * self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.generate_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return steps_per_epoch\n",
    "    \n",
    "    #def on_epoch_end(self):\n",
    "        #self.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZU1P64Qbb069"
   },
   "outputs": [],
   "source": [
    "class DataTranslator():\n",
    "    def __init__(self, labels, domains):\n",
    "        self.chars_map = {x:idx+1 for idx, x in enumerate(set('abcdefghijklmnopqrstuvwxyz0123456789.-'))}\n",
    "        self.chars_map_rev = {idx+1:x for idx, x in enumerate(set('abcdefghijklmnopqrstuvwxyz0123456789.-'))}\n",
    "        self.labels_map = {x:idx for idx, x in enumerate(set(labels))}\n",
    "        self.labels_map_rev = {idx:x for idx, x in enumerate(set(labels))}\n",
    "        \n",
    "        # Numbers the labels\n",
    "        self.labels_num = [self.labels_map[x] for x in labels]\n",
    "        \n",
    "        # Convert domain names to number of sequences\n",
    "        # (+ pad at 64 because this is the max len of a domain names)\n",
    "        self.domains_seq = [[self.chars_map[y] for y in x] for x in domains]\n",
    "        self.domains_seq = DataTranslator.__pad_seq(self.domains_seq)\n",
    "        \n",
    "        self.domains = domains\n",
    "        self.labels = labels\n",
    "\n",
    "    def __pad_seq(seq):\n",
    "        return tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=64)\n",
    "        \n",
    "    def nb_labels(self):\n",
    "        return len(self.labels_map)\n",
    "    \n",
    "    def make_data(self):\n",
    "        # Create the dataset used for the data generation\n",
    "        X = [[] for i in range(self.nb_labels())]\n",
    "        for i in range(len(self.labels_num)):\n",
    "            X[self.labels_num[i]].append(self.domains_seq[i])\n",
    "        return X\n",
    "            \n",
    "    def get_label_name(self, idx):\n",
    "        return self.labels_map_rev.get(idx)\n",
    "    \n",
    "    def get_label_index(self, label_name):\n",
    "        return self.labels_map.get(label_name, -1)\n",
    "    \n",
    "    def domain_to_vec(self, domain_name):\n",
    "        if len(domain_name) > 64:\n",
    "            raise ValueError(\"domain name should contains less than 64 chars\")\n",
    "        translation = None\n",
    "        try:\n",
    "            translation = [self.chars_map[c] for c in domain_name]\n",
    "        except NameError:\n",
    "            raise ValueError(\"given domain name contains unauthorized chars\")\n",
    "\n",
    "        return DataTranslator.__pad_seq([translation])[0]\n",
    "    \n",
    "    def vec_to_domain(self, vec):\n",
    "        str = ''\n",
    "        for c in vec:\n",
    "            str += self.chars_map_rev.get(c, '')\n",
    "        return str\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vhy04h2EyJx8"
   },
   "outputs": [],
   "source": [
    "labels, domains = zip(*data.get_data(nb_data_to_generate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KmeSLVrDasix"
   },
   "outputs": [],
   "source": [
    "translator = DataTranslator(labels, domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOYjd2sunhgM"
   },
   "outputs": [],
   "source": [
    "X = translator.make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOz-tM6yyJyD"
   },
   "outputs": [],
   "source": [
    "def lossless_triplet_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss function\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor data\n",
    "            positive -- the encodings for the positive data (similar to anchor)\n",
    "            negative -- the encodings for the negative data (different from anchor)\n",
    "    N  --  The number of dimension \n",
    "    beta -- The scaling factor, N is recommended\n",
    "    epsilon -- The Epsilon value to prevent ln(0)\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    N = output_dim\n",
    "    beta = N\n",
    "    epsilon=1e-8\n",
    "    \n",
    "    negative = tf.convert_to_tensor(y_pred[:,0:N])\n",
    "    anchor = tf.convert_to_tensor(y_pred[:,N:N*2]) \n",
    "    positive = tf.convert_to_tensor(y_pred[:,N*2:N*3])\n",
    "    \n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),1)\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),1)\n",
    "    \n",
    "    #Non Linear Values  \n",
    "    \n",
    "    # -ln(-x/N+1)\n",
    "    pos_dist = -tf.log(-tf.divide((pos_dist),beta)+1+epsilon)\n",
    "    neg_dist = -tf.log(-tf.divide((N-neg_dist),beta)+1+epsilon)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = neg_dist + pos_dist\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def build_model(input_dim, embedding_voc_len, alpha=0.25):\n",
    "     # Setting the model input\n",
    "    input_neg = tf.keras.Input(shape=(input_dim,), name='negative') # Input from a different class than the Anchor\n",
    "    input_anc = tf.keras.Input(shape=(input_dim,), name='anchor')   # Input on which comparaison should be done\n",
    "    input_pos = tf.keras.Input(shape=(input_dim,), name='positive') # Input of the same class than the Anchor\n",
    "\n",
    "     # Creation of the Encoder\n",
    "    encoder = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(embedding_voc_len, 64, input_length=input_dim, mask_zero=True),\n",
    "        tf.keras.layers.LSTM(units=64),\n",
    "        tf.keras.layers.Dense(output_dim, activation='sigmoid', name='custom_embedding')\n",
    "    ])\n",
    "    \n",
    "    # Anchor the input with the encoder\n",
    "    encoded_neg = encoder(input_neg)\n",
    "    encoded_anc = encoder(input_anc)\n",
    "    encoded_pos = encoder(input_pos)\n",
    "    \n",
    "    merged = tf.keras.layers.concatenate([encoded_neg, encoded_anc, encoded_pos], axis=-1)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_neg, input_anc, input_pos], outputs=merged)\n",
    "\n",
    "    model.compile(optimizer='adam', loss=lossless_triplet_loss)\n",
    "\n",
    "    return encoder, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r2THk-GHyJyK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/white/venv_ml/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "encoder, model = build_model(64, len(translator.chars_map) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xbl6CO5myJyZ"
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    pred = [None] * len(X)\n",
    "    for i in range(len(X)):\n",
    "        pred[i] = encoder.predict(np.array(X[i]))\n",
    "    return pred\n",
    "\n",
    "def data_initializer(X):\n",
    "    rnd_init = [None] * len(X)\n",
    "    for i in range(len(X)):\n",
    "        rnd_init[i] = np.random.randn(len(X[i]), output_dim)\n",
    "    return rnd_init\n",
    "\n",
    "dgen = DataGenerator(X, predict, data_initializer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_indices = np.random.randint(0, len(translator.labels), nb_embedding_data)\n",
    "x_test_emb = [[translator.domains_seq[i] for i in rnd_indices]]\n",
    "y_test_emb = [translator.labels[i] for i in rnd_indices] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = \"./logs\"\n",
    "embedding_metadata = \"emb_metadata.tsv\"\n",
    "with open(os.path.join(logs_dir, embedding_metadata), \"wb\") as f:\n",
    "    np.savetxt(f, y_test_emb, \"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emb_projector(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, model, X, metadata_path, batch_size=32, freq=1, logs_dir='./logs'):\n",
    "        self.sess = K.get_session()\n",
    "        \n",
    "        self.freq = freq\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.x_std = standardize_input_data(X, model.input_names)\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter(logs_dir)\n",
    "        self.logs_dir = logs_dir\n",
    "        \n",
    "        self.metadata_path = metadata_path\n",
    "        \n",
    "    def set_model(self, model, layer):\n",
    "        self.model = model\n",
    "        \n",
    "        self.batch_id = batch_id = tf.placeholder(tf.int32)\n",
    "        self.step = step = tf.placeholder(tf.int32)\n",
    "        \n",
    "        # --- Creation of the embedding layer ---\n",
    "        emb_input = layer.output\n",
    "        emb_size = np.prod(emb_input.shape[1:])\n",
    "        emb_input = tf.reshape(emb_input, (step, int(emb_size)))\n",
    "\n",
    "        shape = (self.x_std[0].shape[0], int(emb_size)) \n",
    "        self.emb_var = tf.Variable(tf.zeros(shape), name=layer.name + \"_embedding\")\n",
    "        self.assign_emb = tf.assign(self.emb_var[batch_id:batch_id+step], emb_input)\n",
    "        \n",
    "        self.saver = tf.train.Saver([self.emb_var])\n",
    "        \n",
    "        config = projector.ProjectorConfig()\n",
    "        \n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = self.emb_var.name\n",
    "        embedding.metadata_path = self.metadata_path\n",
    "        \n",
    "        projector.visualize_embeddings(self.writer, config)\n",
    "        \n",
    "        self.sess.run(self.emb_var.initializer)\n",
    "\n",
    "        \n",
    "    def log_emb(self, checkpoint_name=''):\n",
    "        n_samples = self.x_std[0].shape[0]\n",
    "        \n",
    "        i = 0\n",
    "        while i < n_samples:\n",
    "            step = min(self.batch_size, n_samples - i)\n",
    "            batch = slice(i, i+step)\n",
    "            \n",
    "            feed_dict = {self.model.input: self.x_std[0][batch]}\n",
    "            \n",
    "            feed_dict.update({self.batch_id: i, self.step: step})\n",
    "            \n",
    "            self.sess.run(self.assign_emb, feed_dict=feed_dict)\n",
    "            self.saver.save(self.sess,\n",
    "                           os.path.join(self.logs_dir, 'emb_checkpoint_' + checkpoint_name + '.ckpt'))\n",
    "            \n",
    "            i += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_proj = emb_projector(encoder, x_test_emb, embedding_metadata, batch_size_emb, 5, logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExXc4NRVyJyg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/white/venv_ml/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "256/256 [==============================] - 137s 536ms/step - loss: 3.1625\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 137s 537ms/step - loss: 1.6060\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 135s 526ms/step - loss: 1.3134\n",
      "Epoch 4/20\n",
      "256/256 [==============================] - 135s 526ms/step - loss: 1.2455\n",
      "Epoch 5/20\n",
      "256/256 [==============================] - 135s 526ms/step - loss: 1.2216\n",
      "Epoch 6/20\n",
      "256/256 [==============================] - 135s 526ms/step - loss: 1.1312\n",
      "Epoch 7/20\n",
      "256/256 [==============================] - 135s 528ms/step - loss: 1.1651\n",
      "Epoch 8/20\n",
      "256/256 [==============================] - 135s 526ms/step - loss: 1.0976\n",
      "Epoch 9/20\n",
      "256/256 [==============================] - 135s 526ms/step - loss: 1.1057\n",
      "Epoch 10/20\n",
      "256/256 [==============================] - 135s 529ms/step - loss: 1.0606\n",
      "Epoch 11/20\n",
      "256/256 [==============================] - 135s 529ms/step - loss: 1.1224\n",
      "Epoch 12/20\n",
      "256/256 [==============================] - 136s 531ms/step - loss: 1.0466\n",
      "Epoch 13/20\n",
      "256/256 [==============================] - 135s 528ms/step - loss: 1.0334\n",
      "Epoch 14/20\n",
      "256/256 [==============================] - 135s 527ms/step - loss: 1.0005\n",
      "Epoch 15/20\n",
      "256/256 [==============================] - 135s 529ms/step - loss: 0.9871\n",
      "Epoch 16/20\n",
      "256/256 [==============================] - 135s 528ms/step - loss: 1.0113\n",
      "Epoch 17/20\n",
      "256/256 [==============================] - 136s 531ms/step - loss: 1.0184\n",
      "Epoch 18/20\n",
      "256/256 [==============================] - 136s 530ms/step - loss: 1.0443\n",
      "Epoch 19/20\n",
      "256/256 [==============================] - 136s 531ms/step - loss: 1.0116\n",
      "Epoch 20/20\n",
      "256/256 [==============================] - 136s 531ms/step - loss: 1.0175\n"
     ]
    }
   ],
   "source": [
    "def proj_update(epoch, logs):\n",
    "    emb_proj.set_model(encoder,  encoder.get_layer('custom_embedding'))\n",
    "    emb_proj.log_emb(str(epoch))\n",
    "    \n",
    "update_anchors_cb = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: dgen.update())\n",
    "\n",
    "projector_cb = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: proj_update(epoch, logs))\n",
    "\n",
    "\n",
    "learn_history = model.fit_generator(dgen, epochs=nb_epochs, verbose=1, callbacks=[update_anchors_cb, projector_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oh0OSAPGyJyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "256/256 [==============================] - 115s 450ms/step - loss: 0.9992\n",
      "Epoch 2/100\n",
      "256/256 [==============================] - 115s 449ms/step - loss: 0.9654\n",
      "Epoch 3/100\n",
      "256/256 [==============================] - 115s 449ms/step - loss: 0.9579\n",
      "Epoch 4/100\n",
      "256/256 [==============================] - 115s 451ms/step - loss: 0.9520\n",
      "Epoch 5/100\n",
      "256/256 [==============================] - 115s 450ms/step - loss: 0.9455\n",
      "Epoch 6/100\n",
      "256/256 [==============================] - 115s 450ms/step - loss: 0.9396\n",
      "Epoch 7/100\n",
      "256/256 [==============================] - 116s 452ms/step - loss: 0.9366\n",
      "Epoch 8/100\n",
      "256/256 [==============================] - 116s 451ms/step - loss: 0.9284\n",
      "Epoch 9/100\n",
      "256/256 [==============================] - 116s 452ms/step - loss: 0.9227\n",
      "Epoch 10/100\n",
      "256/256 [==============================] - 116s 452ms/step - loss: 0.9139\n",
      "Epoch 11/100\n",
      "256/256 [==============================] - 116s 453ms/step - loss: 0.9101\n",
      "Epoch 12/100\n",
      "256/256 [==============================] - 115s 451ms/step - loss: 0.9030\n",
      "Epoch 13/100\n",
      "256/256 [==============================] - 116s 453ms/step - loss: 0.9053\n",
      "Epoch 14/100\n",
      "256/256 [==============================] - 116s 452ms/step - loss: 0.8991\n",
      "Epoch 15/100\n",
      "256/256 [==============================] - 116s 452ms/step - loss: 0.8924\n",
      "Epoch 16/100\n",
      "256/256 [==============================] - 116s 452ms/step - loss: 0.8920\n",
      "Epoch 17/100\n",
      "256/256 [==============================] - 116s 452ms/step - loss: 0.8880\n",
      "Epoch 18/100\n",
      "256/256 [==============================] - 116s 452ms/step - loss: 0.8822\n",
      "Epoch 19/100\n",
      "256/256 [==============================] - 116s 453ms/step - loss: 0.8792\n",
      "Epoch 20/100\n",
      "256/256 [==============================] - 116s 453ms/step - loss: 0.8776\n",
      "Epoch 21/100\n",
      "256/256 [==============================] - 116s 452ms/step - loss: 0.8699\n",
      "Epoch 22/100\n",
      "256/256 [==============================] - 116s 453ms/step - loss: 0.8692\n",
      "Epoch 23/100\n",
      "256/256 [==============================] - 116s 455ms/step - loss: 0.8647\n",
      "Epoch 24/100\n",
      "256/256 [==============================] - 116s 454ms/step - loss: 0.8657\n",
      "Epoch 25/100\n",
      "256/256 [==============================] - 116s 454ms/step - loss: 0.8559\n",
      "Epoch 26/100\n",
      "256/256 [==============================] - 116s 454ms/step - loss: 0.8552\n",
      "Epoch 27/100\n",
      "256/256 [==============================] - 116s 455ms/step - loss: 0.8525\n",
      "Epoch 28/100\n",
      "256/256 [==============================] - 116s 454ms/step - loss: 0.8463\n",
      "Epoch 29/100\n",
      "256/256 [==============================] - 116s 454ms/step - loss: 0.8466\n",
      "Epoch 30/100\n",
      "256/256 [==============================] - 116s 454ms/step - loss: 0.8423\n",
      "Epoch 31/100\n",
      "256/256 [==============================] - 116s 455ms/step - loss: 0.8398\n",
      "Epoch 32/100\n",
      "256/256 [==============================] - 117s 456ms/step - loss: 0.8398\n",
      "Epoch 33/100\n",
      "256/256 [==============================] - 116s 455ms/step - loss: 0.8340\n",
      "Epoch 34/100\n",
      "256/256 [==============================] - 117s 456ms/step - loss: 0.8320\n",
      "Epoch 35/100\n",
      "256/256 [==============================] - 116s 455ms/step - loss: 0.8332\n",
      "Epoch 36/100\n",
      "256/256 [==============================] - 116s 455ms/step - loss: 0.8343\n",
      "Epoch 37/100\n",
      "256/256 [==============================] - 117s 457ms/step - loss: 0.8299\n",
      "Epoch 38/100\n",
      "256/256 [==============================] - 117s 456ms/step - loss: 0.8274\n",
      "Epoch 39/100\n",
      "256/256 [==============================] - 117s 457ms/step - loss: 0.8241\n",
      "Epoch 40/100\n",
      "256/256 [==============================] - 117s 456ms/step - loss: 0.8508\n",
      "Epoch 41/100\n",
      "256/256 [==============================] - 117s 456ms/step - loss: 0.8716\n",
      "Epoch 42/100\n",
      "256/256 [==============================] - 117s 457ms/step - loss: 0.8429\n",
      "Epoch 43/100\n",
      "256/256 [==============================] - 117s 456ms/step - loss: 0.8316\n",
      "Epoch 44/100\n",
      "256/256 [==============================] - 117s 458ms/step - loss: 0.8275\n",
      "Epoch 45/100\n",
      "256/256 [==============================] - 117s 457ms/step - loss: 0.8234\n",
      "Epoch 46/100\n",
      "256/256 [==============================] - 117s 458ms/step - loss: 0.8233\n",
      "Epoch 47/100\n",
      "256/256 [==============================] - 117s 457ms/step - loss: 0.8188\n",
      "Epoch 48/100\n",
      "256/256 [==============================] - 117s 458ms/step - loss: 0.8176\n",
      "Epoch 49/100\n",
      "256/256 [==============================] - 117s 457ms/step - loss: 0.8173\n",
      "Epoch 50/100\n",
      "256/256 [==============================] - 117s 458ms/step - loss: 0.8144\n",
      "Epoch 51/100\n",
      "256/256 [==============================] - 117s 457ms/step - loss: 0.8145\n",
      "Epoch 52/100\n",
      "256/256 [==============================] - 117s 459ms/step - loss: 0.8108\n",
      "Epoch 53/100\n",
      "256/256 [==============================] - 117s 457ms/step - loss: 0.8123\n",
      "Epoch 54/100\n",
      "256/256 [==============================] - 117s 459ms/step - loss: 0.8108\n",
      "Epoch 55/100\n",
      "256/256 [==============================] - 117s 458ms/step - loss: 0.8090\n",
      "Epoch 56/100\n",
      "256/256 [==============================] - 118s 459ms/step - loss: 0.8094\n",
      "Epoch 57/100\n",
      "256/256 [==============================] - 117s 459ms/step - loss: 0.8070\n",
      "Epoch 58/100\n",
      "256/256 [==============================] - 118s 459ms/step - loss: 0.8070\n",
      "Epoch 59/100\n",
      "256/256 [==============================] - 118s 459ms/step - loss: 0.8057\n",
      "Epoch 60/100\n",
      "256/256 [==============================] - 118s 460ms/step - loss: 0.8057\n",
      "Epoch 61/100\n",
      "256/256 [==============================] - 118s 459ms/step - loss: 0.8042\n",
      "Epoch 62/100\n",
      "256/256 [==============================] - 118s 460ms/step - loss: 0.8054\n",
      "Epoch 63/100\n",
      "256/256 [==============================] - 118s 460ms/step - loss: 0.8036\n",
      "Epoch 64/100\n",
      "256/256 [==============================] - 118s 460ms/step - loss: 0.8023\n",
      "Epoch 65/100\n",
      "256/256 [==============================] - 118s 460ms/step - loss: 0.8010\n",
      "Epoch 66/100\n",
      "256/256 [==============================] - 118s 460ms/step - loss: 0.8031\n",
      "Epoch 67/100\n",
      "256/256 [==============================] - 118s 460ms/step - loss: 0.7990\n",
      "Epoch 68/100\n",
      "256/256 [==============================] - 118s 460ms/step - loss: 0.8132\n",
      "Epoch 69/100\n",
      "256/256 [==============================] - 118s 462ms/step - loss: 0.8001\n",
      "Epoch 70/100\n",
      "256/256 [==============================] - 118s 461ms/step - loss: 0.7997\n",
      "Epoch 71/100\n",
      "256/256 [==============================] - 118s 461ms/step - loss: 0.7992\n",
      "Epoch 72/100\n",
      "256/256 [==============================] - 118s 462ms/step - loss: 0.8013\n",
      "Epoch 73/100\n",
      "256/256 [==============================] - 118s 461ms/step - loss: 0.7998\n",
      "Epoch 74/100\n",
      "256/256 [==============================] - 118s 463ms/step - loss: 0.7994\n",
      "Epoch 75/100\n",
      "256/256 [==============================] - 118s 461ms/step - loss: 0.8010\n",
      "Epoch 76/100\n",
      "256/256 [==============================] - 118s 463ms/step - loss: 0.7958\n",
      "Epoch 77/100\n",
      "256/256 [==============================] - 118s 463ms/step - loss: 0.7949\n",
      "Epoch 78/100\n",
      "256/256 [==============================] - 119s 464ms/step - loss: 0.7999\n",
      "Epoch 79/100\n",
      "256/256 [==============================] - 118s 463ms/step - loss: 0.7973\n",
      "Epoch 80/100\n",
      "256/256 [==============================] - 119s 463ms/step - loss: 0.7942\n",
      "Epoch 81/100\n",
      "256/256 [==============================] - 119s 463ms/step - loss: 0.8022\n",
      "Epoch 82/100\n",
      "256/256 [==============================] - 119s 463ms/step - loss: 0.8068\n",
      "Epoch 83/100\n",
      "256/256 [==============================] - 119s 465ms/step - loss: 0.7952\n",
      "Epoch 84/100\n",
      "256/256 [==============================] - 119s 464ms/step - loss: 0.7965\n",
      "Epoch 85/100\n",
      "256/256 [==============================] - 119s 463ms/step - loss: 0.7958\n",
      "Epoch 86/100\n",
      "256/256 [==============================] - 119s 465ms/step - loss: 0.7973\n",
      "Epoch 87/100\n",
      "256/256 [==============================] - 119s 464ms/step - loss: 0.7989\n",
      "Epoch 88/100\n",
      "256/256 [==============================] - 119s 465ms/step - loss: 0.7972\n",
      "Epoch 89/100\n",
      "256/256 [==============================] - 119s 464ms/step - loss: 0.7934\n",
      "Epoch 90/100\n",
      "256/256 [==============================] - 119s 466ms/step - loss: 0.8022\n",
      "Epoch 91/100\n",
      "256/256 [==============================] - 119s 464ms/step - loss: 0.7944\n",
      "Epoch 92/100\n",
      "256/256 [==============================] - 119s 465ms/step - loss: 0.7954\n",
      "Epoch 93/100\n",
      "256/256 [==============================] - 120s 467ms/step - loss: 0.7973\n",
      "Epoch 94/100\n",
      "256/256 [==============================] - 119s 466ms/step - loss: 0.7939\n",
      "Epoch 95/100\n",
      "256/256 [==============================] - 119s 466ms/step - loss: 0.7930\n",
      "Epoch 96/100\n",
      "256/256 [==============================] - 119s 465ms/step - loss: 0.7967\n",
      "Epoch 97/100\n",
      "256/256 [==============================] - 120s 468ms/step - loss: 0.8014\n",
      "Epoch 98/100\n",
      "256/256 [==============================] - 119s 467ms/step - loss: 0.7935\n",
      "Epoch 99/100\n",
      "256/256 [==============================] - 119s 466ms/step - loss: 0.7932\n",
      "Epoch 100/100\n",
      "256/256 [==============================] - 120s 467ms/step - loss: 0.7939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1f61790d68>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(dgen, epochs=100, verbose=1, callbacks=[projector_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlW9Pa6WyJy5"
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "nb_pred = 100\n",
    "for data in dgen.data:\n",
    "    pred.append(encoder.predict([data[:nb_pred]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bx_qLfutyJzB"
   },
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "encoder.save_weights('model_weights.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(encoder.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SWDrxZkyJzI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean  : 9.382664\n",
      "median: 9.359338\n",
      "max   : 10.671581\n",
      "min   : 8.670323\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "u,v=5,1\n",
    "\n",
    "for i in range(nb_pred):\n",
    "    for j in range(nb_pred):\n",
    "        l.append(pairwise_distances([pred[u][i]], [pred[v][j]])[0][0])\n",
    "l = np.array(l)\n",
    "print(\"mean  :\", np.mean(l))\n",
    "print(\"median:\", np.median(l))\n",
    "print(\"max   :\", np.max(l))\n",
    "print(\"min   :\", np.min(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0lSg3E7175J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of learn_tf_text.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
